# configs/diayn.yaml
# DIAYN (Diversity is All You Need) Configuration
# Reference: https://arxiv.org/abs/1802.06070

# ===== Environment Configuration =====
env_id: "MiniGrid-Empty-8x8-v0"  # MiniGrid environment ID
obs_type: "rgb"  # Observation type: "rgb" (3-channel) or "grid" (single-channel)

# Available MiniGrid environments (uncomment to use):
# - "MiniGrid-Empty-5x5-v0"
# - "MiniGrid-Empty-8x8-v0"
# - "MiniGrid-Empty-16x16-v0"
# - "MiniGrid-DoorKey-5x5-v0"
# - "MiniGrid-DoorKey-8x8-v0"
# - "MiniGrid-FourRooms-v0"

# ===== Agent Configuration =====
agent:
  # Observation and action spaces (will be auto-filled)
  obs_shape: [7, 7, 3]  # [height, width, channels] - will be overridden
  action_dim: 7  # MiniGrid action space size - will be overridden
  
  # Skill configuration
  skill_dim: 8  # Number of discrete skills to learn
  
  # Network architecture
  hidden_dim: 64  # Hidden layer size for all networks
  
  # Training hyperparameters
  lr: 3e-4  # Learning rate
  gamma: 0.99  # Discount factor
  entropy_coef: 0.1  # Entropy coefficient for policy gradient
  
  # Replay buffer
  batch_size: 128  # Batch size for training
  replay_size: 10000  # Maximum replay buffer size
  
  # Intrinsic reward scaling
  intrinsic_reward_scale: 1.0  # Scale factor for intrinsic rewards
  
  # Device configuration
  device: "auto"  # "auto", "cpu", or "cuda"

# ===== Training Configuration =====
training:
  # Training procedure
  max_episodes: 1000  # Maximum number of training episodes
  max_steps_per_episode: 200  # Maximum steps per episode
  
  # Logging and evaluation
  log_interval: 10  # Log metrics every N episodes
  eval_interval: 50  # Evaluate every N episodes
  eval_episodes: 5  # Number of evaluation episodes
  
  # Checkpointing
  save_interval: 100  # Save model every N episodes
  
  # Early stopping (optional)
  early_stop_reward: None  # Stop training if average reward exceeds this value
  patience: 100  # Number of episodes to wait before early stopping

# ===== Logging Configuration =====
logging:
  log_dir: "logs"  # Base directory for logs
  project_name: "skill-discovery"  # Project name for experiment tracking
  use_tensorboard: true  # Enable TensorBoard logging
  save_video: false  # Save video of agent's performance
  video_interval: 100  # Save video every N episodes

# ===== Notes =====
# 1. obs_shape and action_dim will be automatically set based on the environment
# 2. For best results, adjust batch_size and replay_size based on available memory
# 3. Increase skill_dim for more diverse behaviors, but training will be slower
# 4. Monitor training progress using TensorBoard: `tensorboard --logdir=logs`