{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:45:59.824447Z",
     "iopub.status.busy": "2025-07-27T19:45:59.824148Z",
     "iopub.status.idle": "2025-07-27T19:49:11.165030Z",
     "shell.execute_reply": "2025-07-27T19:49:11.164100Z",
     "shell.execute_reply.started": "2025-07-27T19:45:59.824424Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install \"gymnasium>=1.2.0\" \"imageio>=2.37.0\" \"lightning>=2.5.2\" \"matplotlib>=3.10.3\" \"minigrid>=3.0.0\" \"pyyaml>=6.0.2\" \"minigrid\" \"seaborn>=0.13.2\" \"tensorboard>=2.20.0\" \"torch>=2.7.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:31:10.386660Z",
     "iopub.status.busy": "2025-07-27T19:31:10.386383Z",
     "iopub.status.idle": "2025-07-27T19:31:10.391391Z",
     "shell.execute_reply": "2025-07-27T19:31:10.390595Z",
     "shell.execute_reply.started": "2025-07-27T19:31:10.386637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")\n",
    "    \n",
    "    \n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    \n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class SkillDiscriminator(nn.Module):\n",
    "    \"\"\"Skill discriminator for DIAYN that classifies which skill was used.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, skill_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, skill_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_reward(self, state: torch.Tensor, skill: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the intrinsic reward for the given state and skill.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(state)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            return (log_probs * skill).sum(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "class MiniGridEncoder(BaseModel):\n",
    "    \"\"\"Encoder for the MiniGrid environment So that it can be used in the DIAYN agent\n",
    "    \n",
    "    Args:\n",
    "        input_shape (Tuple[int]): The shape of the input observation.\n",
    "        hidden_size (int): The size of the hidden layer.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The encoded observation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_shape: Tuple[int], feature_dim: int = 64, obs_type: str = \"rgb\"):\n",
    "        super().__init__()\n",
    "        self.obs_type = obs_type\n",
    "        self.obs_shape = obs_shape\n",
    "        \n",
    "        # Determine input channels based on observation type\n",
    "        self.in_channels = 3 if obs_type == \"rgb\" else 1\n",
    "        \n",
    "        # CNN architecture for processing observations\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input with correct shape (N, C, H, W)\n",
    "            dummy = torch.zeros(1, self.in_channels, *obs_shape[:2])\n",
    "            conv_out = self.conv(dummy)\n",
    "            self.conv_output_dim = conv_out.shape[1]\n",
    "            \n",
    "\n",
    "        self.fc = nn.Linear(self.conv_output_dim, feature_dim)\n",
    "        self.feature_dim = feature_dim\n",
    "    \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the encoder\n",
    "        \n",
    "        Args:\n",
    "            obs (torch.Tensor): The input observation of shape (batch, H, W, C) or (H, W, C)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The encoded observation of shape (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Ensure we have a batch dimension\n",
    "        if len(obs.shape) == 3:  # (H, W, C) -> (1, H, W, C)\n",
    "            obs = obs.unsqueeze(0)\n",
    "            \n",
    "        # Convert to float and normalize if needed\n",
    "        if obs.dtype == torch.uint8:\n",
    "            obs = obs.float() / 255.0\n",
    "            \n",
    "        # Convert from NHWC to NCHW format expected by PyTorch\n",
    "        if obs.shape[-1] in [1, 3]:  # If channels are last\n",
    "            obs = obs.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "            \n",
    "        # Ensure we have the right number of channels\n",
    "        if self.obs_type == 'rgb' and obs.shape[1] != 3:\n",
    "            if obs.shape[1] == 1:  # If grayscale, repeat to 3 channels\n",
    "                obs = obs.repeat(1, 3, 1, 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected 1 or 3 channels for RGB, got {obs.shape[1]} channels\")\n",
    "                \n",
    "\n",
    "        x = self.conv(obs)\n",
    "        return torch.relu(self.fc(x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:30:58.384829Z",
     "iopub.status.busy": "2025-07-27T19:30:58.384104Z",
     "iopub.status.idle": "2025-07-27T19:30:58.390838Z",
     "shell.execute_reply": "2025-07-27T19:30:58.390090Z",
     "shell.execute_reply.started": "2025-07-27T19:30:58.384802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 22:43:16.294927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753742596.317975     469 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753742596.324927     469 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Optional\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class BaseAgent(nn.Module, ABC):\n",
    "    \"\"\"Base class for all agents.\n",
    "    \n",
    "    This base class provides common functionality for all agents, including\n",
    "    device management and basic training utilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any], writer: Optional[SummaryWriter] = None):\n",
    "        \"\"\"Initialize the base agent.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary for the agent\n",
    "            writer: TensorBoard SummaryWriter for logging\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.writer = writer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def act(self, obs: Dict[str, Any], deterministic: bool = False) -> Any:\n",
    "        \"\"\"Select an action given an observation.\n",
    "        \n",
    "        Args:\n",
    "            obs: Observation from the environment\n",
    "            deterministic: Whether to sample deterministically\n",
    "            \n",
    "        Returns:\n",
    "            Action to take\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move model to device and update self.device.\"\"\"\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "        \n",
    "    def log_scalar(self, tag: str, value: float, step: int):\n",
    "        \"\"\"Log a scalar value to TensorBoard.\n",
    "        \n",
    "        Args:\n",
    "            tag: Tag for the scalar\n",
    "            value: Value to log\n",
    "            step: Current step for x-axis\n",
    "        \"\"\"\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(tag, value, step)\n",
    "    \n",
    "    def log_histogram(self, tag: str, values: torch.Tensor, step: int):\n",
    "        \"\"\"Log a histogram to TensorBoard.\n",
    "        \n",
    "        Args:\n",
    "            tag: Tag for the histogram\n",
    "            values: Values to create histogram from\n",
    "            step: Current step for x-axis\n",
    "        \"\"\"\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_histogram(tag, values, step)\n",
    "    \n",
    "    def log_model_graph(self, model: nn.Module, sample_input: Any):\n",
    "        \"\"\"Log model graph to TensorBoard.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to log\n",
    "            sample_input: Sample input for the model\n",
    "        \"\"\"\n",
    "        if self.writer is not None:\n",
    "            try:\n",
    "                self.writer.add_graph(model, sample_input)\n",
    "                self.writer.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to log model graph: {e}\")\n",
    "    \n",
    "    def save_checkpoint(self, path: str, **kwargs):\n",
    "        \"\"\"Save model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save checkpoint to\n",
    "            **kwargs: Additional items to save in checkpoint\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'config': self.config,\n",
    "            **kwargs\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, path: str, **kwargs):\n",
    "        \"\"\"Load model from checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to checkpoint file\n",
    "            **kwargs: Additional arguments to pass to model constructor\n",
    "            \n",
    "        Returns:\n",
    "            Loaded model instance\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        config = {**checkpoint['config'], **kwargs}\n",
    "        model = cls(config)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIAYN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ec778f9f-36be-4a40-ab30-f24f17d91a8b",
    "_uuid": "dc47ed0e-c3be-4110-9b27-b6785df12089",
    "execution": {
     "iopub.execute_input": "2025-07-27T19:31:52.168489Z",
     "iopub.status.busy": "2025-07-27T19:31:52.168199Z",
     "iopub.status.idle": "2025-07-27T19:31:52.199611Z",
     "shell.execute_reply": "2025-07-27T19:31:52.198940Z",
     "shell.execute_reply.started": "2025-07-27T19:31:52.168468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                       ('state', 'action', 'skill', 'next_state', 'done', 'reward'))\n",
    "\n",
    "\n",
    "class DIAYNAgent(BaseAgent):\n",
    "    \"\"\"Diversity is All You Need (DIAYN) agent implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any], writer: Optional[SummaryWriter] = None, log_dir: Optional[str] = None):\n",
    "        \"\"\"Initialize the DIAYN agent.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary containing agent parameters\n",
    "            writer: TensorBoard SummaryWriter for logging\n",
    "            log_dir: Directory for logging\n",
    "        \"\"\"\n",
    "        super().__init__(config, writer)\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        # Environment parameters\n",
    "        self.obs_shape = config[\"obs_shape\"]\n",
    "        self.action_dim = config[\"action_dim\"]\n",
    "        self.skill_dim = config.get(\"skill_dim\", 8)\n",
    "        \n",
    "        # Training parameters\n",
    "        self.lr = float(config.get(\"lr\", 3e-4))\n",
    "        self.gamma = float(config.get(\"gamma\", 0.99))\n",
    "        self.batch_size = int(config.get(\"batch_size\", 64))\n",
    "        self.replay_size = int(config.get(\"replay_size\", 10000))\n",
    "        self.entropy_coef = float(config.get(\"entropy_coef\", 0.01))\n",
    "        \n",
    "        # Initialize models\n",
    "        self.encoder = MiniGridEncoder(\n",
    "            self.obs_shape,\n",
    "            feature_dim=config.get(\"hidden_dim\", 64)\n",
    "        )\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(self.encoder.feature_dim + self.skill_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.action_dim)\n",
    "        )\n",
    "        \n",
    "        # Discriminator network\n",
    "        self.discriminator = SkillDiscriminator(\n",
    "            input_dim=self.encoder.feature_dim,\n",
    "            skill_dim=self.skill_dim,\n",
    "            hidden_dim=config.get(\"hidden_dim\", 256)\n",
    "        )\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer_d = torch.optim.AdamW(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.optimizer_p = torch.optim.AdamW(\n",
    "            list(self.encoder.parameters()) + list(self.policy.parameters()),\n",
    "            lr=self.lr,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        # Learning rate schedulers\n",
    "        self.scheduler_d = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer_d, T_max=1000\n",
    "        )\n",
    "        self.scheduler_p = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer_p, T_max=1000\n",
    "        )\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = deque(maxlen=self.replay_size)\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, obs: torch.Tensor, skill: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the agent.\"\"\"\n",
    "        encoded = self.encoder(obs)\n",
    "        x = torch.cat([encoded, skill], dim=-1)\n",
    "        return self.policy(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def act(self, obs: Dict[str, Any], skill: np.ndarray, deterministic: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Select an action given an observation and skill.\n",
    "        Args:\n",
    "            obs: Observation from the environment (can be dict or array-like)\n",
    "            skill: Skill vector (numpy array)\n",
    "            deterministic: Whether to sample deterministically\n",
    "            \n",
    "        Returns:\n",
    "            Action to take\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Handle both dictionary and array observations\n",
    "                if isinstance(obs, dict) and 'observation' in obs:\n",
    "                    obs_data = obs['observation']\n",
    "                else:\n",
    "                    obs_data = obs\n",
    "                    \n",
    "                # Convert to tensor and ensure correct shape and device\n",
    "                obs_tensor = torch.as_tensor(obs_data, dtype=torch.float32, device=self.device)\n",
    "                if obs_tensor.dim() == 3:  # If image, add batch dimension\n",
    "                    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "                    \n",
    "                # Convert skill to tensor\n",
    "                skill_tensor = torch.as_tensor(skill, dtype=torch.float32, device=self.device)\n",
    "                if skill_tensor.dim() == 1:  # Add batch dimension if needed\n",
    "                    skill_tensor = skill_tensor.unsqueeze(0)\n",
    "                \n",
    "                # Get action logits\n",
    "                logits = self.forward(obs_tensor, skill_tensor)\n",
    "                \n",
    "                # Check for invalid logits\n",
    "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                    print(\"Warning: Invalid logits detected, using random action\")\n",
    "                    return np.random.randint(0, self.action_dim)\n",
    "                \n",
    "                if deterministic:\n",
    "                    action = torch.argmax(logits, dim=-1)\n",
    "                else:\n",
    "                    # Clamp logits for numerical stability\n",
    "                    logits = torch.clamp(logits, min=-10, max=10)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Check for invalid probabilities\n",
    "                    if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "                        print(\"Warning: Invalid probabilities detected, using uniform distribution\")\n",
    "                        probs = torch.ones_like(logits) / logits.shape[-1]\n",
    "                    \n",
    "                    action = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                return action.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in act(): {e}\")\n",
    "                # Return random action as fallback\n",
    "                return np.random.randint(0, self.action_dim)\n",
    "        \n",
    "    def update(self, batch: Dict[str, torch.Tensor], step: int) -> Tuple[float, float]:\n",
    "        \"\"\"Update the agent's parameters using a batch of experiences.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch of transitions\n",
    "            step: Current training step (for logging)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (discriminator_loss, policy_loss)\n",
    "        \"\"\"\n",
    "        # Unpack batch\n",
    "        states = batch[\"state\"].to(self.device)\n",
    "        actions = batch[\"action\"].to(self.device)\n",
    "        skills = batch[\"skill\"].to(self.device)\n",
    "        next_states = batch[\"next_state\"].to(self.device)\n",
    "        dones = batch[\"done\"].to(self.device)\n",
    "        \n",
    "        # Train discriminator\n",
    "        with torch.amp.autocast(device_type=self.device.type, enabled=self.device.type == 'cuda'):\n",
    "            # Encode states\n",
    "            states_enc = self.encoder(states)\n",
    "            next_states_enc = self.encoder(next_states).detach()\n",
    "            \n",
    "            # Train discriminator\n",
    "            logits = self.discriminator(next_states_enc)\n",
    "            target = skills.argmax(dim=-1)\n",
    "            if target.max() >= logits.size(1):\n",
    "                raise ValueError(f\"Bad skill index {target.max()} vs {logits.size(1)}\")\n",
    "\n",
    "            loss_d = F.cross_entropy(logits, target)\n",
    "            \n",
    "            # Train policy\n",
    "            policy_input = torch.cat([states_enc, skills], dim=-1)\n",
    "            logits = self.policy(policy_input)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            entropy = -(probs * log_probs).sum(dim=-1)\n",
    "            \n",
    "            # Compute intrinsic reward\n",
    "            with torch.no_grad():\n",
    "                pred_skill_probs = F.softmax(self.discriminator(next_states_enc), dim=-1)\n",
    "                log_pred_skill_probs = torch.log(pred_skill_probs + 1e-6)\n",
    "                intrinsic_reward = (log_pred_skill_probs * skills).sum(dim=-1)\n",
    "                \n",
    "            # Compute policy loss\n",
    "            policy_loss = -(log_probs.gather(1, actions.unsqueeze(1)) * intrinsic_reward.unsqueeze(1)).mean()\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            loss_p = policy_loss + entropy_loss\n",
    "            \n",
    "        # Update discriminator\n",
    "        self.optimizer_d.zero_grad()\n",
    "        loss_d.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 0.5)\n",
    "        self.optimizer_d.step()\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer_p.zero_grad()\n",
    "        loss_p.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.encoder.parameters()) + list(self.policy.parameters()),\n",
    "            0.5\n",
    "        )\n",
    "        self.optimizer_p.step()\n",
    "        \n",
    "        # Update learning rates\n",
    "        self.scheduler_d.step()\n",
    "        self.scheduler_p.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar('train/discriminator_loss', loss_d.item(), step)\n",
    "            self.writer.add_scalar('train/policy_loss', loss_p.item(), step)\n",
    "            self.writer.add_scalar('train/entropy', entropy.mean().item(), step)\n",
    "            self.writer.add_scalar('train/intrinsic_reward', intrinsic_reward.mean().item(), step)\n",
    "            \n",
    "            # Log learning rates\n",
    "            self.writer.add_scalar('lr/discriminator', self.scheduler_d.get_last_lr()[0], step)\n",
    "            self.writer.add_scalar('lr/policy', self.scheduler_p.get_last_lr()[0], step)\n",
    "        \n",
    "        return loss_d.item(), loss_p.item()\n",
    "    \n",
    "    def add_to_replay(self, transition: Transition) -> None:\n",
    "        \"\"\"Add a transition to the replay buffer.\"\"\"\n",
    "        self.replay_buffer.append(transition)\n",
    "    \n",
    "    def sample_batch(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the replay buffer.\n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample\n",
    "        Returns:\n",
    "            Dictionary containing batched tensors for states, actions, skills, next_states, dones, rewards.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "\n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        states = torch.stack([torch.FloatTensor(s) for s in batch.state])\n",
    "        actions = torch.LongTensor(batch.action)\n",
    "        skills = torch.FloatTensor(np.array(batch.skill))\n",
    "        next_states = torch.stack([torch.FloatTensor(s) for s in batch.next_state])\n",
    "        dones = torch.FloatTensor(batch.done)\n",
    "        rewards = torch.FloatTensor(batch.reward)\n",
    "\n",
    "        return {\n",
    "            'state': states,\n",
    "            'action': actions,\n",
    "            'skill': skills,\n",
    "            'next_state': next_states,\n",
    "            'done': dones,\n",
    "            'reward': rewards\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, path: str) -> None:\n",
    "        \"\"\"Save agent state to checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'encoder_state_dict': self.encoder.state_dict(),\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'optimizer_d_state_dict': self.optimizer_d.state_dict(),\n",
    "            'optimizer_p_state_dict': self.optimizer_p.state_dict(),\n",
    "            'scheduler_d_state_dict': self.scheduler_d.state_dict(),\n",
    "            'scheduler_p_state_dict': self.scheduler_p.state_dict(),\n",
    "            'replay_buffer': self.replay_buffer,\n",
    "            'config': self.config\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path: str) -> None:\n",
    "        \"\"\"Load agent state from checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "        self.optimizer_p.load_state_dict(checkpoint['optimizer_p_state_dict'])\n",
    "        self.scheduler_d.load_state_dict(checkpoint['scheduler_d_state_dict'])\n",
    "        self.scheduler_p.load_state_dict(checkpoint['scheduler_p_state_dict'])\n",
    "        self.replay_buffer = checkpoint['replay_buffer']\n",
    "        self.config = checkpoint['config']\n",
    "        \n",
    "        # Move models to device\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def log_model_graph(self) -> None:\n",
    "        if self.writer is None:\n",
    "            return\n",
    "        if not all(hasattr(self, attr) for attr in ['encoder', 'policy', 'discriminator']):\n",
    "            print(\"Graph logging skipped: model parts not initialized.\")\n",
    "            return\n",
    "\n",
    "        dummy_obs = torch.zeros((1, *self.obs_shape), device=self.device)\n",
    "        dummy_skill = torch.zeros((1, self.skill_dim), device=self.device)\n",
    "\n",
    "        # Policy wrapper\n",
    "        class PolicyWrapper(nn.Module):\n",
    "            def __init__(self, agent):\n",
    "                super().__init__()\n",
    "                self.encoder = agent.encoder\n",
    "                self.policy = agent.policy\n",
    "            def forward(self, obs, skill):\n",
    "                encoded = self.encoder(obs)\n",
    "                return self.policy(torch.cat([encoded, skill], dim=-1))\n",
    "\n",
    "        # Discriminator wrapper\n",
    "        class DiscriminatorWrapper(nn.Module):\n",
    "            def __init__(self, agent):\n",
    "                super().__init__()\n",
    "                self.encoder = agent.encoder\n",
    "                self.discriminator = agent.discriminator\n",
    "            def forward(self, obs):\n",
    "                return self.discriminator(self.encoder(obs))\n",
    "\n",
    "        # Write policy graph in its own run\n",
    "        writer_policy = SummaryWriter(log_dir=os.path.join(self.log_dir, \"policy_graph\"))\n",
    "        writer_policy.add_graph(PolicyWrapper(self), (dummy_obs, dummy_skill))\n",
    "        writer_policy.flush()\n",
    "\n",
    "        # Write discriminator graph in separate run\n",
    "        writer_disc = SummaryWriter(log_dir=os.path.join(self.log_dir, \"discriminator_graph\"))\n",
    "        writer_disc.add_graph(DiscriminatorWrapper(self), (dummy_obs,))\n",
    "        writer_disc.flush()\n",
    "\n",
    "    \n",
    "    def sample_skill(self) -> np.ndarray:\n",
    "        \"\"\"Sample a random one-hot skill vector.\"\"\"\n",
    "        skill_idx = np.random.randint(0, self.skill_dim)\n",
    "        skill = np.zeros(self.skill_dim, dtype=np.float32)\n",
    "        skill[skill_idx] = 1.0\n",
    "        return skill\n",
    "    \n",
    "    def train(self, mode: bool = True) -> 'DIAYNAgent':\n",
    "        \"\"\"Set the agent in training mode.\"\"\"\n",
    "        super().train(mode)\n",
    "        self.encoder.train(mode)\n",
    "        self.policy.train(mode)\n",
    "        self.discriminator.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def eval(self) -> 'DIAYNAgent':\n",
    "        \"\"\"Set the agent in evaluation mode.\"\"\"\n",
    "        return self.train(False)\n",
    "    \n",
    "    def to(self, device) -> 'DIAYNAgent':\n",
    "        \"\"\"Move the agent to the specified device.\"\"\"\n",
    "        super().to(device)\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.policy = self.policy.to(device)\n",
    "        self.discriminator = self.discriminator.to(device)\n",
    "        return self\n",
    "    \n",
    "    def store_transition(self, transition: Transition) -> None:\n",
    "        self.replay_buffer.append(transition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:41:14.884078Z",
     "iopub.status.busy": "2025-07-27T19:41:14.883377Z",
     "iopub.status.idle": "2025-07-27T19:41:14.889261Z",
     "shell.execute_reply": "2025-07-27T19:41:14.888382Z",
     "shell.execute_reply.started": "2025-07-27T19:41:14.884052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"diayn.yaml\",\"w\") as f :\n",
    "    f.write(\"\"\"\n",
    "# configs/diayn.yaml\n",
    "# DIAYN (Diversity is All You Need) Configuration\n",
    "# Reference: https://arxiv.org/abs/1802.06070\n",
    "\n",
    "# ===== Environment Configuration =====\n",
    "env_id: \"MiniGrid-Empty-8x8-v0\"  # MiniGrid environment ID\n",
    "obs_type: \"rgb\"  # Observation type: \"rgb\" (3-channel) or \"grid\" (single-channel)\n",
    "\n",
    "# Available MiniGrid environments (uncomment to use):\n",
    "# - \"MiniGrid-Empty-5x5-v0\"\n",
    "# - \"MiniGrid-Empty-8x8-v0\"\n",
    "# - \"MiniGrid-Empty-16x16-v0\"\n",
    "# - \"MiniGrid-DoorKey-5x5-v0\"\n",
    "# - \"MiniGrid-DoorKey-8x8-v0\"\n",
    "# - \"MiniGrid-FourRooms-v0\"\n",
    "\n",
    "# ===== Agent Configuration =====\n",
    "agent:\n",
    "  # Observation and action spaces (will be auto-filled)\n",
    "  obs_shape: [7, 7, 3]  # [height, width, channels] - will be overridden\n",
    "  action_dim: 7  # MiniGrid action space size - will be overridden\n",
    "  \n",
    "  # Skill configuration\n",
    "  skill_dim: 8  # Number of discrete skills to learn\n",
    "  \n",
    "  # Network architecture\n",
    "  hidden_dim: 512  # Hidden layer size for all networks\n",
    "  \n",
    "  # Training hyperparameters\n",
    "  lr: 1e-5  # Learning rate\n",
    "  gamma: 0.99  # Discount factor\n",
    "  entropy_coef: 0.1  # Entropy coefficient for policy gradient\n",
    "  \n",
    "  # Replay buffer\n",
    "  batch_size: 32768 # Batch size for training\n",
    "  replay_size: 50000  # Maximum replay buffer size\n",
    "  \n",
    "  # Intrinsic reward scaling\n",
    "  intrinsic_reward_scale: 1.0  # Scale factor for intrinsic rewards\n",
    "  \n",
    "  # Device configuration\n",
    "  device: \"auto\"  # \"auto\", \"cpu\", or \"cuda\"\n",
    "\n",
    "# ===== Training Configuration =====\n",
    "training:\n",
    "  # Training procedure\n",
    "  max_episodes: 1000000 # Maximum number of training episodes\n",
    "  max_steps_per_episode: 1000  # Maximum steps per episode\n",
    "  \n",
    "  # Logging and evaluation\n",
    "  log_interval: 10000  # Log metrics every N episodes\n",
    "  eval_interval: 50000  # Evaluate every N episodes\n",
    "  eval_episodes: 5000  # Number of evaluation episodes\n",
    "  \n",
    "  # Checkpointing\n",
    "  save_interval: 10000  # Save model every N episodes\n",
    "  \n",
    "  # Early stopping (optional)\n",
    "  early_stop_reward: None  # Stop training if average reward exceeds this value\n",
    "  patience: 100  # Number of episodes to wait before early stopping\n",
    "\n",
    "  #Envirement \n",
    "  env_parallel: 4\n",
    "# ===== Logging Configuration =====\n",
    "logging:\n",
    "  log_dir: \"logs\"  # Base directory for logs\n",
    "  project_name: \"skill-discovery\"  # Project name for experiment tracking\n",
    "  use_tensorboard: true  # Enable TensorBoard logging\n",
    "  save_video: false  # Save video of agent's performance\n",
    "  video_interval: 1000  # Save video every N episodes\n",
    "\n",
    "# ===== Notes =====\n",
    "# 1. obs_shape and action_dim will be automatically set based on the environment\n",
    "# 2. For best results, adjust batch_size and replay_size based on available memory\n",
    "# 3. Increase skill_dim for more diverse behaviors, but training will be slower\n",
    "# 4. Monitor training progress using TensorBoard: `tensorboard --logdir=logs`\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinitGrid init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:43:26.938725Z",
     "iopub.status.busy": "2025-07-27T19:43:26.938455Z",
     "iopub.status.idle": "2025-07-27T19:43:26.944261Z",
     "shell.execute_reply": "2025-07-27T19:43:26.943538Z",
     "shell.execute_reply.started": "2025-07-27T19:43:26.938705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Register MiniGrid environments with Gymnasium.\"\"\"\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# Register all available MiniGrid environments\n",
    "register(\n",
    "    id=\"MiniGrid-Empty-5x5-v0\",\n",
    "    entry_point=\"minigrid.envs:EmptyEnv\",\n",
    "    kwargs={\"size\": 5}\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-Empty-8x8-v0\",\n",
    "    entry_point=\"minigrid.envs:EmptyEnv\",\n",
    "    kwargs={\"size\": 8}\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-Empty-16x16-v0\",\n",
    "    entry_point=\"minigrid.envs:EmptyEnv\",\n",
    "    kwargs={\"size\": 16}\n",
    ")\n",
    "\n",
    "# You can add more environments as needed\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-5x5-v0\",\n",
    "    entry_point=\"minigrid.envs:DoorKeyEnv\",\n",
    "    kwargs={\"size\": 5}\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-8x8-v0\",\n",
    "    entry_point=\"minigrid.envs:DoorKeyEnv\",\n",
    "    kwargs={\"size\": 8}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGrid_Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:43:39.034970Z",
     "iopub.status.busy": "2025-07-27T19:43:39.034445Z",
     "iopub.status.idle": "2025-07-27T19:43:39.042192Z",
     "shell.execute_reply": "2025-07-27T19:43:39.041404Z",
     "shell.execute_reply.started": "2025-07-27T19:43:39.034947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "\n",
    "class MiniGridWrapper(gym.Wrapper):\n",
    "    \"\"\"Wrapper for the MiniGrid environment\"\"\"\n",
    "    def __init__(self,env,skill_dim=8,obs_type=\"rgb\"):\n",
    "        super().__init__(env)\n",
    "        self.skill_dim = skill_dim\n",
    "        self.obs_type = obs_type\n",
    "        \n",
    "        if obs_type == \"rgb\":\n",
    "            self.obs_shape = (7,7,3)\n",
    "        else: #obs_type = grid\n",
    "            self.obs_shape = (7,7)\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"observation\": spaces.Box(\n",
    "                low=0,high=255,\n",
    "                shape=self.obs_shape,\n",
    "                dtype=np.uint8\n",
    "            ),\n",
    "            \"skill\": spaces.Box(\n",
    "                low=-1.0,high=1.0,\n",
    "                shape=(skill_dim,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    def reset(self,**kwargs):\n",
    "        obs,info = super().reset(**kwargs)\n",
    "        return self._process_obs(obs),info\n",
    "    \n",
    "    def step(self,action):\n",
    "        obs,reward,terminated,truncated,info = self.env.step(action)\n",
    "        return self._process_obs(obs),reward,terminated,truncated,info\n",
    "    \n",
    "    def _process_obs(self,obs):\n",
    "        \"\"\"\n",
    "        Process the observation to match the observation space\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.obs_type == \"rgb\":\n",
    "            obs_array = obs[\"image\"][...,:3]\n",
    "        else:\n",
    "            obs_array = obs[\"image\"][...,0]\n",
    "        \n",
    "        skill = np.random.uniform(-1,1,size=(self.skill_dim,))\n",
    "        \n",
    "        return {\n",
    "            \"observation\":obs_array,\n",
    "            \"skill\":skill\n",
    "        }\n",
    "        \n",
    "\n",
    "class ObservationExtractor(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = self.env.observation_space['observation']\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return obs['observation']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                       ('state', 'action', 'skill', 'next_state', 'done', 'reward'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, default=\"/kaggle/working/diayn.yaml\")\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=\"logs\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_rollout(env: MiniGridWrapper, agent: DIAYNAgent, max_steps: int = 1000) -> Tuple[float, int]:\n",
    "    \"\"\"Collect a single rollout from the environment with optimized data transfer.\n",
    "    Args:\n",
    "        env: The environment to collect the rollout from\n",
    "        agent: The agent to select actions\n",
    "        max_steps: Maximum number of steps per episode\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing total reward and episode length\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    skill = agent.sample_skill()\n",
    "    episode_reward = 0.0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    # print(\"Episode length: \", episode_length)\n",
    "    # print(\"Max steps: \", max_steps)\n",
    "    # print(episode_length < max_steps)\n",
    "    while not done and episode_length < max_steps:\n",
    "        \n",
    "        action = agent.act(obs, skill)        \n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        transition = Transition(state=obs['observation'] if isinstance(obs, dict) and 'observation' in obs else obs,\n",
    "                                action=action,\n",
    "                                skill=skill,\n",
    "                                next_state=next_obs['observation'] if isinstance(next_obs, dict) and 'observation' in next_obs else next_obs,\n",
    "                                done=done,\n",
    "                                reward=reward)\n",
    "        agent.add_to_replay(transition)\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "    return episode_reward, episode_length\n",
    "\n",
    "\n",
    "\n",
    "def collect_parallel_rollout(env, agent: DIAYNAgent, max_steps: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Collect a rollout using vectorized environments.\n",
    "    \n",
    "    Args:\n",
    "        env: Vectorized environment (SyncVectorEnv or AsyncVectorEnv)\n",
    "        agent: DIAYN agent\n",
    "        max_steps: Maximum number of steps per episode\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (total_rewards, episode_lengths) arrays per environment\n",
    "    \"\"\"\n",
    "    num_envs = env.num_envs\n",
    "    obs, _ = env.reset()\n",
    "    skills = np.array([agent.sample_skill() for _ in range(num_envs)], dtype=np.float32)\n",
    "\n",
    "    episode_rewards = np.zeros(num_envs, dtype=np.float32)\n",
    "    episode_lengths = np.zeros(num_envs, dtype=np.int32)\n",
    "    dones = np.zeros(num_envs, dtype=bool)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        actions = []\n",
    "        for i in range(num_envs):\n",
    "            if not dones[i]:\n",
    "                action = agent.act(obs[i], skills[i])\n",
    "            else:\n",
    "                action = 0  # Dummy action for already-done envs\n",
    "            actions.append(action)\n",
    "\n",
    "        actions = np.array(actions)\n",
    "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        step_dones = np.logical_or(terminations, truncations)\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            if dones[i]:  # Skip if already done\n",
    "                continue\n",
    "\n",
    "            transition = Transition(\n",
    "                state=obs[i]['observation'] if isinstance(obs[i], dict) else obs[i],\n",
    "                action=actions[i],\n",
    "                skill=skills[i],\n",
    "                next_state=next_obs[i]['observation'] if isinstance(next_obs[i], dict) else next_obs[i],\n",
    "                done=step_dones[i],\n",
    "                reward=rewards[i]\n",
    "            )\n",
    "            agent.add_to_replay(transition)\n",
    "\n",
    "            episode_rewards[i] += rewards[i]\n",
    "            episode_lengths[i] += 1\n",
    "\n",
    "        dones = np.logical_or(dones, step_dones)\n",
    "        obs = next_obs\n",
    "\n",
    "        if np.all(dones):\n",
    "            break\n",
    "\n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(env, agent: DIAYNAgent, num_episodes: int = 5, episode: int = 0, writer: Optional[SummaryWriter] = None) -> float:\n",
    "    \"\"\"Evaluate the agent's performance on a vectorized environment.\"\"\"\n",
    "    agent.eval()\n",
    "    eval_rewards = []\n",
    "    eval_lengths = []\n",
    "    \n",
    "    episodes_collected = 0\n",
    "    num_envs = env.num_envs\n",
    "\n",
    "    while episodes_collected < num_episodes:\n",
    "        rewards, lengths = collect_parallel_rollout(env, agent, max_steps=1000)\n",
    "        \n",
    "        for r ,l in zip(rewards, lengths):\n",
    "            eval_rewards.append(r)\n",
    "            eval_lengths.append(l)\n",
    "            episodes_collected += 1\n",
    "            if episodes_collected >= num_episodes:\n",
    "                break\n",
    "\n",
    "    avg_eval_reward = np.mean(eval_rewards)\n",
    "    if writer:\n",
    "        writer.add_scalar(\"eval/avg_reward\", avg_eval_reward, episode)\n",
    "        writer.add_scalar(\"eval/avg_length\", np.mean(eval_lengths), episode)\n",
    "    \n",
    "    agent.train()\n",
    "    return avg_eval_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envirement Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.vector import SyncVectorEnv\n",
    "\n",
    "\n",
    "def make_env(env_id, obs_type):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        env = MiniGridWrapper(env, obs_type=obs_type)\n",
    "        env = ObservationExtractor(env)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:41:44.428972Z",
     "iopub.status.busy": "2025-07-27T19:41:44.428642Z",
     "iopub.status.idle": "2025-07-27T19:41:44.461297Z",
     "shell.execute_reply": "2025-07-27T19:41:44.460546Z",
     "shell.execute_reply.started": "2025-07-27T19:41:44.428949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def train():\n",
    "    sys.argv = ['']\n",
    "    args = parse_args()\n",
    "    config = load_config(args.config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    \n",
    "    print(f\"Using device: {device} | Device  Name: {device_name}\")\n",
    "    seed = config.get(\"seed\", 42)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create  vectorized environments\n",
    "    num_envs = config.get(\"training\", {}).get(\"env_parallel\", 1)\n",
    "    env_thunks = [make_env(config[\"env_id\"], config.get(\"obs_type\", \"rgb\")) for _ in range(num_envs)]\n",
    "    print(env_thunks[0])\n",
    "    env = SyncVectorEnv(env_thunks) \n",
    "\n",
    "    sample_obs = env.reset()[0]  # env.reset() returns (obs, info)\n",
    "    print(sample_obs.shape)\n",
    "    config[\"agent\"][\"obs_shape\"] = sample_obs.shape[1:]\n",
    "    config[\"agent\"][\"action_dim\"] = env.action_space.nvec[0]\n",
    "    \n",
    "\n",
    "    # Create unique log directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = os.path.join(args.log_dir, f\"diayn_{timestamp}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize TensorBoard writer with the correct log directory\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(f\"TensorBoard logs will be saved to: {os.path.abspath(log_dir)}\")\n",
    "\n",
    "\n",
    "    agent = DIAYNAgent(config[\"agent\"], writer=writer, log_dir=log_dir).to(device)\n",
    "    agent.log_model_graph()\n",
    "\n",
    "    training_cfg = config.get(\"training\", {})\n",
    "    print(training_cfg.keys())\n",
    "    num_episodes = training_cfg.get(\"max_episodes\", 1000)\n",
    "    eval_interval = training_cfg.get(\"eval_interval\", 100)\n",
    "    save_interval = training_cfg.get(\"save_interval\", 100)\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = -float('inf')\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Training progress bar\n",
    "    pbar = tqdm(range(1, num_episodes + 1), desc=\"Training\")\n",
    "    \n",
    "    for episode in pbar:\n",
    "        try:\n",
    "            # Clear CUDA cache periodically\n",
    "            if episode % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during episode {episode}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "        \n",
    "        # Perform training step\n",
    "        if len(agent.replay_buffer) >= agent.batch_size:\n",
    "            try:\n",
    "                batch = agent.sample_batch(agent.batch_size)\n",
    "                if batch is None:\n",
    "                    continue\n",
    "\n",
    "                # Update both discriminator and policy\n",
    "                loss_d, loss_p = agent.update(batch, episode)\n",
    "                \n",
    "                if writer is not None:\n",
    "                    writer.add_scalar('train/discriminator_loss', loss_d, episode)\n",
    "                    writer.add_scalar('train/policy_loss', loss_p, episode)\n",
    "                    writer.add_scalar('train/replay_buffer_size', len(agent.replay_buffer), episode)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error during training step: {str(e)}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()  # Wait for all kernels to finish\n",
    "            \n",
    "        # Perform evaluation\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            _avg_eval_reward = evaluate(env, agent, num_episodes=5, episode=episode, writer=writer)\n",
    "        \n",
    "\n",
    "        if (episode + 1) % save_interval == 0 or episode == num_episodes - 1:\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"diayn_episode_{episode+1}.ckpt\")\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'model_state_dict': agent.state_dict(),\n",
    "                'optimizer_d_state_dict': agent.optimizer_d.state_dict(),\n",
    "                'optimizer_p_state_dict': agent.optimizer_p.state_dict(),\n",
    "                'episode_rewards': episode_rewards,\n",
    "                'episode_lengths': episode_lengths,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "\n",
    "    final_model_path = os.path.join(checkpoint_dir, \"diayn_final.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': agent.state_dict(),\n",
    "        'optimizer_d_state_dict': agent.optimizer_d.state_dict(),\n",
    "        'optimizer_p_state_dict': agent.optimizer_p.state_dict(),\n",
    "        'config': config\n",
    "    }, final_model_path)\n",
    "    print(f\"\\nTraining complete! Final model saved to {final_model_path}\")\n",
    "    \n",
    "\n",
    "    metrics = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    metrics_path = os.path.join(log_dir, 'training_metrics.pt')\n",
    "    torch.save(metrics, metrics_path)\n",
    "    print(f\"Training metrics saved to {metrics_path}\")\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T19:49:23.802210Z",
     "iopub.status.busy": "2025-07-27T19:49:23.801908Z",
     "iopub.status.idle": "2025-07-27T20:05:37.146516Z",
     "shell.execute_reply": "2025-07-27T20:05:37.145946Z",
     "shell.execute_reply.started": "2025-07-27T19:49:23.802184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda | Device  Name: Tesla P100-PCIE-16GB\n",
      "<function make_env.<locals>._thunk at 0x7ac3a5169a80>\n",
      "(4, 7, 7, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-DoorKey-5x5-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-DoorKey-8x8-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-Empty-5x5-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-Empty-8x8-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment MiniGrid-Empty-16x16-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: /kaggle/working/logs/diayn_20250728_224321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_469/2069167454.py:63: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if obs.shape[-1] in [1, 3]:  # If channels are last\n",
      "/tmp/ipykernel_469/2069167454.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.obs_type == 'rgb' and obs.shape[1] != 3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['max_episodes', 'max_steps_per_episode', 'log_interval', 'eval_interval', 'eval_episodes', 'save_interval', 'early_stop_reward', 'patience', 'env_parallel'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%| | 849999/1000000 [00:41<00:07, 20965.64it/s]/tmp/ipykernel_469/4168527256.py:259: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.FloatTensor(batch.done)\n",
      "Training:  85%| | 850313/1000000 [05:22<29:47:10,  1.40it/s]"
     ]
    }
   ],
   "source": [
    "agent = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minigrid\n",
      "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from minigrid) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from minigrid) (0.29.0)\n",
      "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from minigrid) (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->minigrid) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->minigrid) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->minigrid) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->minigrid) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->minigrid) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->minigrid) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.0->minigrid) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.0->minigrid) (2024.2.0)\n",
      "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: minigrid\n",
      "Successfully installed minigrid-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir /kaggle/working/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
